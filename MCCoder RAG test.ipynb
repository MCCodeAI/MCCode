{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC Code RAG Workshop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector base and RAG test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p1=0  #Parameter 1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai chromadb bs4 python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key from .env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "\n",
    "# Use this line of code if you have a local .env file\n",
    "load_dotenv(find_dotenv()) \n",
    "\n",
    "# Or set it like this\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "\n",
    "# Print this line to double check your API key\n",
    "# print(os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangSmith(enabled by default)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls__7cb2a18f629e4c10b5a34d49223fba88\n"
     ]
    }
   ],
   "source": [
    "# Store all the keys in .env\n",
    "import getpass\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.environ[\"LANGCHAIN_API_KEY\"] #getpass.getpass()\n",
    "print(os.environ[\"LANGCHAIN_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import from\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader, TextLoader, PyPDFLoader, PyPDFium2Loader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web loader\n",
    "# loader = WebBaseLoader(\n",
    "#     web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "#     bs_kwargs=dict(\n",
    "#         parse_only=bs4.SoupStrainer(\n",
    "#             class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "#         )\n",
    "#     ),\n",
    "# )\n",
    "# docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pdf loader\n",
    "file_path = './docs/WMX3SampleCodes.pdf'\n",
    "# file_path = 'nais2023.pdf'\n",
    "# loader = PyPDFLoader(file_path)\n",
    "# loader = PyPDFLoader(file_path, extract_images=True)   #extract images as text as well\n",
    "\n",
    "loader = PyPDFium2Loader(file_path,  extract_images=False) \n",
    "# docs = loader.load_and_split()\n",
    "docs = loader.load()\n",
    "print(docs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Txt loader\n",
    "loader = TextLoader(\"./docs/WMX3UserManual_a.txt\")\n",
    "docs = loader.load()\n",
    "# docs[0].page_content[:100000]\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Txt loader\n",
    "loader = TextLoader(\"./docs/WMX3SampleCodes.py\")\n",
    "docs = loader.load()\n",
    "# docs[0].page_content[:100000]\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Txt loader for api with instruction\n",
    "loader = TextLoader(\"./docs/WMX3API_CleanedData.json\")\n",
    "docs = loader.load()\n",
    "# docs[0].page_content[:100000]\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WMX3manual text chunk \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample code chunk with `` separators\n",
    "separators = ['``']  # Adjust based on actual document structure\n",
    "text_splitter = RecursiveCharacterTextSplitter(separators=separators, keep_separator=True, chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample code chunk with } separators -api with instruction, WMX3API_CleanedData.json\n",
    "separators = ['}']  # Adjust based on actual document structure\n",
    "text_splitter = RecursiveCharacterTextSplitter(separators=separators, keep_separator=True, chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Main \\n\\n\\nThis help file contains information regarding the functions that are available in WMX3. \\nUse the links to the left to navigate through this help file. \\n \\n\\nSupport Specifications \\n\\n\\nWMX3 RTX Version \\n\\n \\n\\n\\nWMX3 RTX Version \\n\\n\\nSupported PC Hardware \\nSupported OS \\nSupported NIC \\nSupported Library and IDE \\nSupported Sample Project \\nSupport for user application development running only on a real-time OS \\nSupport for user application development running only on a non real-time \\nOS \\n \\n\\n\\nSupported PC Hardware \\n\\nTo operate WMX3, a PC with the following specifications is required. \\n\\nCPU \\nRequirement', metadata={'source': './docs/WMX3UserManual_a.txt', 'start_index': 0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Supported PC Hardware \\n\\nTo operate WMX3, a PC with the following specifications is required. \\n\\nCPU \\nRequirement \\n\\n\\nPerformance : A CPU with performance of at least the Atom E3845 (2GHz) is \\nrequired. \\nNumber of cores : Since it is necessary to assign a dedicated CPU core to RTX, a \\nminimum of 2 cores (excluding Hyper-Threading virtual cores) is required. \\nGeneration : Each version of RTX supports specific CPU generations. Select the \\nCPU based on the compatibility information below. CPUs that are not listed in the \\ncompatibility information are not supported at that time. \\nFor additional information, please refer to the following technical note of IntervalZero \\n(the developer of RTX). \\n\\nhttp://softservo.com/pub/RTX_Doc/RTX64/Manual/RTX64_Processor_Compatibility.pdf \\n\\nRecommended', metadata={'source': './docs/WMX3UserManual_a.txt', 'start_index': 492})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(RecursiveCharacterTextSplitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorstore - chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model=OpenAIEmbeddings(model=\"text-embedding-3-large\")   #text-embedding-3-large   #text-embedding-ada-002    #text-embedding-3-small\n",
    "\n",
    "# If txt vectorstore exists\n",
    "# if os.path.exists(\"Vectorstore/chromadb-MCCoder\"):\n",
    "        # vectorstore = Chroma(\n",
    "                #     embedding_function=embedding_model,\n",
    "                #     persist_directory=\"Vectorstore/chromadb\",\n",
    "                #     ) \n",
    "# else:\n",
    "        # Load from chunks and save to disk\n",
    "        # vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model, persist_directory=\"Vectorstore/chromadb\") \n",
    "\n",
    "# If vectorstore exists\n",
    "vectorstore_path = \"Vectorstore/chromadb-MCCoder\"\n",
    " # Load from chunks and save to disk\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model,  persist_directory=vectorstore_path) \n",
    "# if os.path.exists(vectorstore_path):\n",
    "#         vectorstore = Chroma(\n",
    "#                     embedding_function=embedding_model,\n",
    "#                     persist_directory=vectorstore_path,\n",
    "#                     ) \n",
    "# else:\n",
    "#         # Load from chunks and save to disk\n",
    "#         vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model, persist_directory=vectorstore_path) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "# retriever = vectorstore.similarity_search_with_score('a typical python code of WMX3 for a axis/servo/motor to move or do positioning.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieved_docs = retriever.invoke(\"a typical python code of WMX3 for a axis/servo/motor to move or do positioning. \")\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"Supported PC Hardware \")\n",
    "\n",
    "#print(retrieved_docs[0].page_content)\n",
    "retrieved_docs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorstore - pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "index_name = os.environ[\"PINECONE_INDEX_NAME\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PineconeVectorStore.delete(delete_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docsearch = PineconeVectorStore.from_documents(splits, embedding_model, index_name=index_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = PineconeVectorStore.from_existing_index(index_name, embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is the Architecture of wmx3?\"\n",
    "retrieved_docs = docsearch.similarity_search(query,k=8)\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorstore - FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model=OpenAIEmbeddings(model=\"text-embedding-3-large\")   \n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstore_path = \"Vectorstore/FAISS-pdf-images\"\n",
    "db = FAISS.from_documents(splits, embedding_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.save_local(folder_path=\"Vectorstore/FAISS-pdf-images\", index_name=\"myFaissIndex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.load_local(folder_path=\"Vectorstore/FAISS-pdf-images\",embeddings=embedding_model,index_name=\"myFaissIndex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"a typical python code of WMX3 for a axis/servo/motor to move or do positioning.\"\n",
    "docs = db.similarity_search(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorstore - pathway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### serve Maximum marginal relevance search (MMR) Similarity search by vector ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "qdrant = Qdrant.from_documents(\n",
    "    splits,\n",
    "    embedding_model,\n",
    "    path=\"Vectorstore/Qdrant-pdf-images\",  # Local mode with in-memory storage only\n",
    "    collection_name=\"my_documents\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_rag_prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rag chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.1)   #gpt-4  #gpt-3.5-turbo\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rag_chain.invoke(\"write an example code to close wmx3?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in rag_chain.stream(\"write a sample code to initialize wmx3?\"): \n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-demo-IMu3vKF7-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
