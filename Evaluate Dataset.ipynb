{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate code with various llms and send to WMX3 running for log and plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from disk: Vectorstore/chromadb-MCCoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tasks:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task ID: 1 ðŸ”½\n",
      "codeerr:\n",
      "Traceback (most recent call last):\n",
      "  File \"\\\\mac\\Home\\Downloads\\codedemo\\sample.py\", line 300, in <module>\n",
      "    main()\n",
      "  File \"\\\\mac\\Home\\Downloads\\codedemo\\sample.py\", line 204, in main\n",
      "    ret = WMX3Log.SetCustomLog(0)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: Log.SetCustomLog() missing 1 required positional argument: 'input'\n",
      "!!!\n",
      "----------------------\n",
      "\n",
      "\n",
      "error string:----------------------\n",
      "TypeError: Log.SetCustomLog() missing 1 required positional argument: 'input'\n",
      "\n",
      "error ref:----------------------\n",
      "{\n",
      "        \"No\": 699,\n",
      "        \"FunctionPython\": \"def SetLog(channel, input)\\u00a0\",\n",
      "        \"FunctionC++\": \"WMX3APIFUNC SetLog(unsigned int channel, LogInput *input)\",\n",
      "        \"Parameters\": \"[in] channel The channel of the log operation. Each channel operates independently from all other channels. The first channel is 0. The number of available channels is defined by the maxLogChannel constant. \\n[in] input A pointer to an object of a class that inherits the LogInput class. \\n\",\n",
      "        \"Remarks\": \"This function specifies which data to be collected by the logging operation.\\n\\nEach module that contains data that can be logged defines a class that inherits the LogInput class. For example, the CoreMotion module defines the CoreMotionLogInput class, which can be used to collect axis data such as position commands. The IO module defines the IoLogInput, which can be used to collect I/O data.\\n\\nData from multiple modules may be logged simultaneously. To do so, call this function multiple times, passing in a different LogInput object each time. For example, to collect the position command data of an axis and I/O data in a single log file, call this function passing an CoreMotionLogInput object, and then call this function again passing an IoLogInput object.\\n\\nIf the data to collect from a module has been specified already, calling this function again for the same module will overwrite the set of data to collect.\\n\\nThis function allocates memory each time it is called. If there is not enough contiguous memory in the real time operating system space, an error will be returned.\\n\\nThe memory allocated by this function is released when the log operation finishes. To start another log operation, this function must be called again.\\n\",\n",
      "        \"ReturnType\": \"int\\u00a0\",\n",
      "        \"ReturnValue\": \"error code\\u00a0\",\n",
      "        \"Class\": \"log Class\",\n",
      "        \"Instruction\": \"Specify data to be collected by the logging operation.\"\n",
      "    },\n",
      "    \n",
      "\n",
      "{\n",
      "        \"No\": 710,\n",
      "        \"FunctionPython\": \"def SetIOLog(channel, pInputIOAddress, inputSize, pOutputIOAddress, outputSize)\\u00a0\",\n",
      "        \"FunctionC++\": \"WMX3APIFUNC SetIOLog(unsigned int channel, IOAddress *pInputIOAddress, unsigned int inputSize, IOAddress *pOutputIOAddress, unsigned int outputSize)\",\n",
      "        \"Parameters\": \"[in] channel The channel of the log operation. Each channel operates independently from all other channels. The first channel is 0. The number of available channels is defined by the maxLogChannel constant. \\n[in] pInputIOAddress An array of I/O input addresses. The size parameter must be specified. Multiple contiguous bits can be specified by setting size to the number of bits. \\n[in] inputSize The number elements in the pInputIOAddress array. \\n[in] pOutputIOAddress An array of I/O output addresses. The size parameter must be specified. Multiple contiguous bits can be specified by setting size to the number of bits. \\n[in] outputSize The number of elements in the pOutputIOAddress array. \\n\",\n",
      "        \"Remarks\": \"This function can be called to specify the data log function to log I/O data when StartLog is called. I/O data can be logged alongside position/velocity data by calling both SetLog and SetIOLog. \",\n",
      "        \"ReturnType\": \"int\\u00a0\",\n",
      "        \"ReturnValue\": \"error code\\u00a0\",\n",
      "        \"Class\": \"log Class\",\n",
      "        \"Instruction\": NaN\n",
      "    },\n",
      "    \n",
      "\n",
      "{\n",
      "        \"No\": 708,\n",
      "        \"FunctionPython\": \"def SetLog(channel, pPath, milliseconds, samplePeriodInCycles, pAxisSelection, pOptions, mode)\",\n",
      "        \"FunctionC++\": \"WMX3APIFUNC SetLog(unsigned int channel, char *pPath, unsigned int milliseconds, unsigned int samplePeriodInCycles, AxisSelection *pAxisSelection, LogOptions *pOptions,unsigned int mode, unsigned int burstWriteLines=4, unsigned int scale=9)\",\n",
      "        \"Parameters\": \"[in] channel The channel of the log operation. Each channel operates independently from all other channels. The first channel is 0. The number of available channels is defined by the maxLogChannel constant. \\n[in] input A pointer to an object of a class that inherits the LogInput class. \\n\",\n",
      "        \"Remarks\": \"This function specifies which data to be collected by the logging operation.\\n\\nEach module that contains data that can be logged defines a class that inherits the LogInput class. For example, the CoreMotion module defines the CoreMotionLogInput class, which can be used to collect axis data such as position commands. The IO module defines the IoLogInput, which can be used to collect I/O data.\\n\\nData from multiple modules may be logged simultaneously. To do so, call this function multiple times, passing in a different LogInput object each time. For example, to collect the position command data of an axis and I/O data in a single log file, call this function passing an CoreMotionLogInput object, and then call this function again passing an IoLogInput object.\\n\\nIf the data to collect from a module has been specified already, calling this function again for the same module will overwrite the set of data to collect.\\n\\nThis function allocates memory each time it is called. If there is not enough contiguous memory in the real time operating system space, an error will be returned.\\n\\nThe memory allocated by this function is released when the log operation finishes. To start another log operation, this function must be called again.\\n\",\n",
      "        \"ReturnType\": \"int\\u00a0\",\n",
      "        \"ReturnValue\": \"error code\\u00a0\",\n",
      "        \"Class\": \"log Class\",\n",
      "        \"Instruction\": \"Specify data to be collected by the logging operation.\"\n",
      "    },\n",
      "    \n",
      "\n",
      "{\n",
      "        \"No\": 711,\n",
      "        \"FunctionPython\": \"def SetIOLogFormat(channel, pInputIOLogFormat, inputIOFormatCount, pOutputIOLogFormat, outputIOFormatCount)\\u00a0\",\n",
      "        \"FunctionC++\": \"WMX3APIFUNC SetIOLogFormat(unsigned int channel, IOLogFormat *pInputIOLogFormat, unsigned int inputIOFormatCount, IOLogFormat *pOutputIOLogFormat, unsigned int outputIOFormatCount)\",\n",
      "        \"Parameters\": \"[in] channel The channel of the log operation. Each channel operates independently from all other channels. The first channel is 0. The number of available channels is defined by the maxLogChannel constant. \\n[in] pInputIOLogFormat A pointer to an array of IOLogFormat objects. Each IOLogFormat object defines a contiguous region of I/O input addresses to log using a particular format. The byte member specifies the starting byte address of the region of I/O inputs to log. The bit member specifies the starting bit address of the region of I/O inputs to log. The ioFormatType member specifies the format of the logged data. \\n[in] inputIOFormatCount The number of elements in the inputIOLogFormat array. \\n[in] pOutputIOLogFormat A pointer to an array of IOLogFormat objects. Each IOLogFormat object defines a contiguous region of I/O output addresses to log using a particular format. The byte member specifies the starting byte address of the region of I/O outputs to log. The bit member specifies the starting bit address of the region of I/O outputs to log. The ioFormatType member specifies the format of the logged data. \\n[in] outputIOFormatCount The number of elements in the outputIOLogFormat array. \\n\",\n",
      "        \"Remarks\": \"This function is an alternate way to log I/O data. This function allows contiguous sequences of I/O to be logged under a single column, whereas the SetIOLog function logs each individual I/O bit under a separate column. This function is useful when logging I/O data for systems which contain entities that are represented by multiple I/O bits, such as analog I/O modules. \",\n",
      "        \"ReturnType\": \"int\\u00a0\",\n",
      "        \"ReturnValue\": \"error code\\u00a0\",\n",
      "        \"Class\": \"log Class\",\n",
      "        \"Instruction\": NaN\n",
      "    },\n",
      "    \n",
      "\n",
      "{\n",
      "        \"No\": 697,\n",
      "        \"FunctionPython\": \"def SetLogHeader(channel, pLogHeader)\\u00a0\",\n",
      "        \"FunctionC++\": \"WMX3APIFUNC SetLogHeader(unsigned int channel, char **ppLine, unsigned int numLines)\",\n",
      "        \"Parameters\": \"[in] channel The channel of the log operation. Each channel operates independently from all other channels. The first channel is 0. The number of available channels is defined by the maxLogChannel constant. \\n[in] ppLine An array of pointers to null-terminated char arrays. Each null-terminated char array contains one line of the header. \\n[in] numLines The number of elements (pointers to null-terminated char arrays) in the value passed to the \\\"ppLine\\\" parameter. Each pointer (to a null-terminated char array) contains one line of the header string, so the \\\"numLines\\\" value effectively specifies the number of lines in the header. \\n\",\n",
      "        \"Remarks\": \"This function sets the header string that is printed at the beginning of the log file. This function is optional, and it is not necessary to call this function when collecting log data. If this function is not called, no header will be printed at the beginning of the log file.\\n\\nThe maximum number of lines that may constitute the header is defined by the maxLogHeaderLines constant. The maximum number of characters total in the header (including the new line character at the end of each line) is defined by the maxLogHeaderBytes constant.\\n\\nThe log header can be removed by calling the ResetLog function. All other log settings and options will also be cleared, and will need to be set again. \\n\",\n",
      "        \"ReturnType\": \"int\\u00a0\",\n",
      "        \"ReturnValue\": \"error code\\u00a0\",\n",
      "        \"Class\": \"log Class\",\n",
      "        \"Instruction\": \"Set the log header string that is copied to the beginning of the log file.\"\n",
      "    },\n",
      "    \n",
      "\n",
      "{\n",
      "        \"No\": 709,\n",
      "        \"FunctionPython\": NaN,\n",
      "        \"FunctionC++\": \"WMX3APIFUNC SetLog(unsigned int channel, wchar_t *pPath, unsigned int milliseconds, unsigned int samplePeriodInCycles, AxisSelection *pAxisSelection, LogOptions *pOptions,unsigned int mode, unsigned int burstWriteLines=4, unsigned int scale=9)\",\n",
      "        \"Parameters\": NaN,\n",
      "        \"Remarks\": NaN,\n",
      "        \"ReturnType\": NaN,\n",
      "        \"ReturnValue\": NaN,\n",
      "        \"Class\": \"log Class\",\n",
      "        \"Instruction\": \"Specify data to be collected by the logging operation.\"\n",
      "    },\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tasks:  10%|â–ˆ         | 1/10 [01:16<11:24, 76.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codeerr:\n",
      "  File \"\\\\mac\\Home\\Downloads\\codedemo\\sample.py\", line 200\n",
      "    It appears that you have provided a description of two functions related to logging in a software or hardware environment. Let's break down each function and their details:\n",
      "IndentationError: unexpected indent\n",
      "!!!\n",
      "----------------------\n",
      "\n",
      "Self-correction but still got an error.\n",
      "\n",
      "\n",
      "Task ID: 2 ðŸ”½\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tasks:  10%|â–ˆ         | 1/10 [01:53<17:04, 113.79s/it]\n"
     ]
    },
    {
     "ename": "ResponseError",
     "evalue": "an unknown error was encountered while running the model ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 574\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[38;5;28mprint\u001b[39m(self_corrected_log_results)\n\u001b[1;32m    570\u001b[0m             file\u001b[38;5;241m.\u001b[39mwrite(self_corrected_log_results)\n\u001b[0;32m--> 574\u001b[0m EvalDataset()\n",
      "Cell \u001b[0;32mIn[34], line 460\u001b[0m, in \u001b[0;36mEvalDataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m    458\u001b[0m         coder_return \u001b[38;5;241m=\u001b[39m coder_retrieval(coder_router_result)  \u001b[38;5;66;03m# Code context\u001b[39;00m\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;66;03m# Call CoderLLM function\u001b[39;00m\n\u001b[0;32m--> 460\u001b[0m         code_from_llm \u001b[38;5;241m=\u001b[39m CoderLLM(user_question, coder_return, task_id)\n\u001b[1;32m    461\u001b[0m         code_from_llm_str \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m#---------task\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:---------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m tasks[i] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m#---------code\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:---------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m code_from_llm \n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# Single task\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[34], line 237\u001b[0m, in \u001b[0;36mCoderLLM\u001b[0;34m(user_question, code_context, task_id)\u001b[0m\n\u001b[1;32m    227\u001b[0m prompt_code \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(prompt_template)\n\u001b[1;32m    229\u001b[0m rag_chain \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#{\"context\": context_msg, \"question\": RunnablePassthrough()}\u001b[39;00m\n\u001b[1;32m    231\u001b[0m     prompt_code\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;241m|\u001b[39m llm\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;241m|\u001b[39m StrOutputParser()\n\u001b[1;32m    234\u001b[0m )\n\u001b[0;32m--> 237\u001b[0m codes \u001b[38;5;241m=\u001b[39m rag_chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: code_context, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_question})\n\u001b[1;32m    239\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/yin/Documents/GitHub/MCCodeLog/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllm_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    240\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllm_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_direct_output.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/runnables/base.py:2875\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2873\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2874\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2875\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m   2876\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2877\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:270\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    267\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    269\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 270\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[1;32m    271\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    272\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    273\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    274\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    275\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    276\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    277\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    278\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    279\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    280\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:703\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    697\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    701\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    702\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 703\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:560\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    559\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 560\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    561\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    562\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    564\u001b[0m ]\n\u001b[1;32m    565\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:550\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    549\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 550\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[1;32m    551\u001b[0m                 m,\n\u001b[1;32m    552\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    553\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    554\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    555\u001b[0m             )\n\u001b[1;32m    556\u001b[0m         )\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:775\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    774\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 775\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m    776\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    777\u001b[0m         )\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    779\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_ollama/chat_models.py:607\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    602\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    606\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[0;32m--> 607\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_stream_with_aggregation(\n\u001b[1;32m    608\u001b[0m         messages, stop, run_manager, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    609\u001b[0m     )\n\u001b[1;32m    610\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m final_chunk\u001b[38;5;241m.\u001b[39mgeneration_info\n\u001b[1;32m    611\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[1;32m    612\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(\n\u001b[1;32m    613\u001b[0m             content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    617\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[1;32m    618\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_ollama/chat_models.py:508\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[0;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    501\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    506\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[1;32m    507\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 508\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_stream(messages, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    509\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stream_resp, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    510\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m ChatGenerationChunk(\n\u001b[1;32m    511\u001b[0m                 message\u001b[38;5;241m=\u001b[39mAIMessageChunk(\n\u001b[1;32m    512\u001b[0m                     content\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    525\u001b[0m                 ),\n\u001b[1;32m    526\u001b[0m             )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/langchain_ollama/chat_models.py:490\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m ollama\u001b[38;5;241m.\u001b[39mchat(\n\u001b[1;32m    481\u001b[0m         model\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    482\u001b[0m         messages\u001b[38;5;241m=\u001b[39mollama_messages,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    487\u001b[0m         tools\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    488\u001b[0m     )\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 490\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m ollama\u001b[38;5;241m.\u001b[39mchat(\n\u001b[1;32m    491\u001b[0m         model\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    492\u001b[0m         messages\u001b[38;5;241m=\u001b[39mollama_messages,\n\u001b[1;32m    493\u001b[0m         stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    494\u001b[0m         options\u001b[38;5;241m=\u001b[39mOptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptions\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m    495\u001b[0m         keep_alive\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeep_alive\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    496\u001b[0m         \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    497\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/ollama/_client.py:89\u001b[0m, in \u001b[0;36mClient._stream\u001b[0;34m(self, method, url, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m partial \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;241m:=\u001b[39m partial\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 89\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m partial\n",
      "\u001b[0;31mResponseError\u001b[0m: an unknown error was encountered while running the model "
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_openai import ChatOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import Runnable\n",
    "from langchain.schema.runnable.config import RunnableConfig\n",
    "\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader, TextLoader, PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings, OpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.chat_models.tongyi import ChatTongyi\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "from time import *\n",
    "\n",
    "from CodeClient import *\n",
    "from make_code_runnable import *\n",
    "from plot_log import *\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "\n",
    "\n",
    "# Global variable to store the name of the LLM\n",
    "llm_name = \"gpt-4o-mini\"\n",
    "llm = ChatOpenAI(name=\"MCCoder and QA\", model_name=llm_name, temperature=0.2, streaming=True)\n",
    "\n",
    "# # Groq\n",
    "# llm_name = \"llama-3.1-8b-instant\"\n",
    "# llm = ChatGroq(\n",
    "#     temperature=0.2,\n",
    "#     model=llm_name)   # llama-3.1-8b-instant,  llama3-70b-8192,  llama-3.1-70b-versatile, llama-3.1-405b-reasoning, mixtral-8x7b-32768\n",
    "\n",
    "# Tongyi Qwen\n",
    "# llm_name = \"qwen-plus\"\n",
    "# llm = ChatTongyi(\n",
    "#     temperature=0.2,\n",
    "#     model=llm_name)   # qwen-turbo(8k), qwen-plus  (32k), qwen-max  (6k),  qwen-max-longcontext (28k)\n",
    "\n",
    "# Ollama\n",
    "# llm_name = \"deepseek-coder-v2\"\n",
    "# llm = ChatOllama(\n",
    "# model=\"deepseek-coder-v2\",            # codellama:7b , codellama:34b, tinyllama, codegeex4, deepseek-coder-v2\n",
    "# temperature=0.2)\n",
    "\n",
    "\n",
    "# Prepare docs for RAG\n",
    "\n",
    "load_dotenv(find_dotenv()) \n",
    "\n",
    "# Preparation of documents for RAG-------------------------\n",
    "# Vectorstore, for retrieval\n",
    "embedding_model=OpenAIEmbeddings(model=\"text-embedding-3-large\")   #text-embedding-3-large   #text-embedding-ada-002    #text-embedding-3-small\n",
    "\n",
    "# Embedding model for Azure OpenAI, no need FQ.\n",
    "# embedding_model = AzureOpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "\n",
    "# If pdf vectorstore exists\n",
    "vectorstore_path = \"Vectorstore/chromadb-MCCoder\"\n",
    "if os.path.exists(vectorstore_path):\n",
    "    vectorstore = Chroma(\n",
    "                    embedding_function=embedding_model,\n",
    "                    persist_directory=vectorstore_path,\n",
    "                    ) \n",
    "    print(\"load from disk: \" + vectorstore_path)\n",
    "else:\n",
    "        # Load from chunks and save to disk\n",
    "    # vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model, persist_directory=vectorstore_path) \n",
    "    print(\"load from chunks\")\n",
    "\n",
    "\n",
    "\n",
    "# Txt loader of sample codes, for BM25 search\n",
    "loader = TextLoader(\"./docs/WMX3API_MCEval_Samplecodes.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "#Sample code chunk with dedicated separators\n",
    "separators = ['``']  # Adjust based on actual document structure, `` is the end of each code snippet.\n",
    "text_splitter = RecursiveCharacterTextSplitter(separators=separators, keep_separator=True, chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Define a global variable user_question_global\n",
    "user_question_global = ''\n",
    "\n",
    "# Extracts and formats code instructions from a user question based on specific starting phrases.\n",
    "def coder_router(user_question):\n",
    "    \"\"\"\n",
    "    Extracts numbered sections of a user question based on specific starting phrases.\n",
    "    \n",
    "    If the question starts with 'Write a python code', 'Python code', or 'write python' (case insensitive),\n",
    "    it splits the question into paragraphs that start with numbers (e.g., 1., 2., 3.) and adds \n",
    "    'Write python code to ' after the numbers. If the question does not start \n",
    "    with the specified phrases or does not contain numbered lists, the entire question is saved into a single \n",
    "    element array. If the question does not start with the specified phrases, NoCoder is set to 1.\n",
    "    \n",
    "    Args:\n",
    "        user_question (str): The user's question.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: NoCoder (int), an array of strings with each element containing a code instruction or the entire question.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    NoCoder = 0\n",
    "    # Check if the input starts with the specified prefixes\n",
    "    if re.match(r'(?i)^(Write a python code|Python code|write python)', user_question):\n",
    "        result.append(user_question)\n",
    "    else:\n",
    "        # Save the entire question to the array and set NoCoder to 1\n",
    "        result.append(user_question)\n",
    "        NoCoder = 1\n",
    "    \n",
    "    return NoCoder, result\n",
    "\n",
    "\n",
    "\n",
    "# This function retrieves and concatenates documents for each element in the input string array.\n",
    "def coder_retrieval(coder_router_result):\n",
    "    \"\"\"\n",
    "    This function takes an array of strings as input. For each element in the array,\n",
    "    it performs a retrieval using format_docs(retriever.invoke(element))\n",
    "    and concatenates the element with the retrieval result into one long string, \n",
    "    with a newline character between them. Each concatenated result is separated by a specified separator.\n",
    "    \n",
    "    Args:\n",
    "        coder_router_result (list): An array of strings.\n",
    "\n",
    "    Returns:\n",
    "        str: A single long string formed by concatenating each element with its retrieval result,\n",
    "             separated by a newline character, and each concatenated result separated by a specified separator.\n",
    "    \"\"\"\n",
    "    separator = \"\\n----------\\n\"\n",
    "    long_string = \"\"\n",
    "    using_basic_rag = False\n",
    "    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "    for element in coder_router_result:\n",
    "        if using_basic_rag == True:\n",
    "            # -------------------------------------------\n",
    "            # Basic retrieval\n",
    "            retrieval_result = format_docs(retriever.invoke(element))\n",
    "        else:\n",
    "            # -------------------------------------------\n",
    "            # Fusion retrieval or hybrid search\n",
    "\n",
    "\n",
    "            # initialize the bm25 retriever  \n",
    "            bm25_retriever = BM25Retriever.from_documents(splits)\n",
    "            bm25_retriever.k = 5\n",
    "\n",
    "            # initialize the ensemble retriever\n",
    "            ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, retriever], weights=[0.5, 0.5])\n",
    "\n",
    "            ensemble_docs = ensemble_retriever.invoke(element)\n",
    "\n",
    "            retrieval_result = format_docs(ensemble_docs)\n",
    "\n",
    "\n",
    "        long_string += element + \"\\n\" + retrieval_result + separator\n",
    "    \n",
    "    return long_string\n",
    "\n",
    "\n",
    "# Joins the page content of each document with double newline\n",
    "def format_docs(docs):\n",
    "   return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Extracts code snippets written in Python from the given text\n",
    "def extract_code(text):\n",
    "    # Define the regular expression pattern to find text between ```python and ```\n",
    "    pattern = r\"```python(.*?)```\"\n",
    "\n",
    "    # Use re.findall to find all occurrences\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    if matches == []: \n",
    "        return text\n",
    "    # Return the matches, join them if there are multiple matches\n",
    "    return \"\\n\\n# ---\\n\\n\".join(matches)\n",
    "\n",
    "\n",
    "# Call LLM to generate code\n",
    "def CoderLLM(user_question, code_context, task_id):\n",
    "\n",
    "    # Prompt for code generation\n",
    "    prompt_template = \"\"\"Write a python code based on the following Question and Context. You need to choose the most relevant sample codes from the Context for a reference. And, note the following situations:\n",
    "    1. Review the Question carefully and only if you find words as 'Axis number', 'IO Input' and 'IO Output'(case insensitive), add them to the first lines of the generated code in the following format: \n",
    "    # Axes = [Axis number 1, Axis number 2, ...]\n",
    "    # Inputs = [byte.bit 1, byte.bit 2, ...]\n",
    "    # Outputs = [byte.bit 1, byte.bit 2, ...]\n",
    "    For instance, if the Question is '...Axis 9..., ...Axis 12..., ...Axis 2..., IO Input 0.3 and 1.2, ...IO Output 3.4 and 6.1', then exact the information after matching the keywords: \"Axis\", \"Input\", \"Output\":\n",
    "    # Axes = [9, 12, 2]\n",
    "    # Inputs = [0.3, 1.2, ...]\n",
    "    # Outputs = [3.4, 6.1, ...]\n",
    "    2. Include all the generated codes within one paragraph between ```python and ``` tags. \n",
    "    3. Don't import any library.\n",
    "    4. Don't create any functions or example usage or unit test.\n",
    "    5. You need to wait until the Axes reaches the target position and stops, after the motion API, unless otherwise specified. For instance, Wmx3Lib_cm.motion.Wait(4), while 4 is the Axis specified in Axes.\n",
    "    6. Use StartPos for absolute positioning, as in 'Move Axis 4 to 200', and StartMov for relative positioning, as in 'Move Axis 4 by a distance of 200'.\n",
    "    7. Strictly follow the Question for the specified profile type.\n",
    "    8. If acceleration/acc, deceleration/dec, and velocity/speed are not specified in the user query, use the default values provided in the context's sample codes.\n",
    "    ----------------------------------------------\n",
    "\n",
    "    Question: \n",
    "    {question}\n",
    "\n",
    "    Context: \n",
    "    {context}\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    prompt_code = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "    rag_chain = (\n",
    "        #{\"context\": context_msg, \"question\": RunnablePassthrough()}\n",
    "        prompt_code\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "\n",
    "    codes = rag_chain.invoke({\"context\": code_context, \"question\": user_question})\n",
    "    \n",
    "    folder_path = f'/Users/yin/Documents/GitHub/MCCodeLog/{llm_name}'\n",
    "    file_name = f\"{task_id}_{llm_name}_direct_output.txt\"\n",
    "    file_path = f\"{folder_path}/{file_name}\"\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Write the direct output codes to the file\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(codes)\n",
    "\n",
    "    # Get python code from the output of LLM\n",
    "    ext_codes = extract_code(codes)\n",
    "\n",
    "    return ext_codes\n",
    "\n",
    "# Corrects the provided error codes based on specified error information calling LLM\n",
    "def self_correct(err_info, original_code):\n",
    "   # Search to get the python function as a context for self correction.\n",
    "    python_function_retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6, \"filter\":{\"source\":\"./docs/WMX3API_FunctionPython.json\"}}) \n",
    "\n",
    "    # Split the string into lines\n",
    "    lines = err_info.split('\\n')\n",
    "    error_str = ''\n",
    "    # Iterate through each line to find 'Error:'\n",
    "    for line in lines:\n",
    "        if 'Error:' in line:\n",
    "            # Assign the line containing 'Error:' to error_str\n",
    "            error_str = line\n",
    "    # error_str = 'GetInBit'\n",
    "    print('\\nerror string:----------------------\\n' + error_str )\n",
    "    python_function_result = python_function_retriever.invoke(error_str)\n",
    "    err_ref = format_docs(python_function_result)\n",
    "    print('\\nerror ref:----------------------\\n' + err_ref)\n",
    "\n",
    "    \n",
    "    \n",
    "   # Remember to write \"python\" code in the prompt later\n",
    "    template = \"\"\"Correct the original code based on the user question, error infomation and FunctionPython reference. And, note the following situations:\n",
    "    1. Only if the error is 'variable_name is not defined', and if the variable_name is in the user question , assigni it a value of None firstly.\n",
    "    2. Only if an error information indicates that acc, dec, velocity, or other arguments are out of range, just assign them the default values presented in the preceding code samples. For instance, xxx.profile.acc = 10000, xxx.profile.dec = 10000.\n",
    "    3. Only if an error information indicates that '... buffer memory has already been allocated...', free the buffer in the beginning.\n",
    "\n",
    "        User question:\n",
    "        {user_question}\n",
    "\n",
    "        Original code:\n",
    "        {original_code}\n",
    "\n",
    "        Error information:\n",
    "        {err_info}\n",
    "\n",
    "        FunctionPython reference:\n",
    "        {err_ref}\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "    \n",
    "    rag_chain = (\n",
    "            # {\"err_codes\": RunnablePassthrough()}\n",
    "            custom_rag_prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    code_corrected=rag_chain.invoke({ \"user_question\": user_question_global, \"original_code\": original_code, \"err_info\": err_info, \"err_ref\": err_ref})\n",
    " \n",
    "    return(code_corrected)\n",
    "\n",
    "\n",
    "# Decompose tasks from user questions using a LLM\n",
    "def task_decomposer_llm(user_question):\n",
    "   #  \n",
    "    template = \"\"\"Only if the user question contains a consecutive numbered list as '1.', '2.', '3.', decompose the tasks; otherwise just output the user question. For example, the user question 'Write Python code to execute the following tasks: 1. Move Axis 1 to 200; 2. Move Axis 9 as a distance of 150; 3. Set IO output 4.3 to 1, and sleep for 1.5 seconds.' should be decomposed into three tasks as output adding 'Write python code to ':\n",
    "    1. Write python code to Move Axis 1 to 200;\n",
    "    2. Write python code to Move Axis 9 as a distance of 150;\n",
    "    3. Write python code to Set IO output 4.3 to 1, and sleep for 1.5 seconds.\n",
    "\n",
    "        User question:\n",
    "        {question}\n",
    "\n",
    "        Output:\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "    \n",
    "    rag_chain = (\n",
    "            # {\"err_codes\": RunnablePassthrough()}\n",
    "            custom_rag_prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    task_str=rag_chain.invoke({\"question\": user_question})\n",
    "    \n",
    "    lines = task_str.splitlines()\n",
    "    tasks = []\n",
    "    \n",
    "    # Check if each line starts with the correct number followed by a period and a space\n",
    "    for i, line in enumerate(lines):\n",
    "        expected_number = f\"{i + 1}.\"\n",
    "        if line.startswith(expected_number):\n",
    "            tasks.append(line.strip())\n",
    "    \n",
    "    # If there are no multiple tasks, just output the original question.\n",
    "    if len(tasks) == 0:\n",
    "        tasks.append(task_str)\n",
    "    \n",
    "    return tasks\n",
    "\n",
    "\n",
    "# Decompose tasks from user questions using a LLM\n",
    "def tasks_composer_llm(user_question, code_from_llm_str):\n",
    "   #  \n",
    "    template = \"\"\"Write a Python code that incorporates the Context_Codes (tasks) to address the following Question:\n",
    "\n",
    "    Question: \n",
    "    {question}\n",
    "\n",
    "    Context_Codes: \n",
    "    {context}\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "    \n",
    "    rag_chain = (\n",
    "            # {\"err_codes\": RunnablePassthrough()}\n",
    "            custom_rag_prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    code_from_composer_llm=rag_chain.invoke({\"questions\": user_question, \"context\": code_from_llm_str})\n",
    "    # Get python code from the output of LLM\n",
    "    code_from_composer_llm = extract_code(code_from_composer_llm)\n",
    "    \n",
    "    return code_from_composer_llm\n",
    "\n",
    "\n",
    "# Send the code generated by the LLM to WMX3 engine\n",
    "def RunCode(codes_from_llm, task_info):\n",
    "\n",
    "    RunnableCode = make_code_runnable(codes_from_llm, llm_name, task_info)\n",
    "    # print(RunnableCode)\n",
    "\n",
    "    # Run Code in WMX3\n",
    "    codereturn = SendCode(RunnableCode)\n",
    "    # If there is an error, invoke llm to self-correct, and then send to WMX3 again.\n",
    "    if 'error' in codereturn.lower():\n",
    "        code_corrected = self_correct(codereturn, codes_from_llm)\n",
    "        msgCode = extract_code(code_corrected)\n",
    "        RunnableCode = make_code_runnable(msgCode, llm_name, task_info)\n",
    "        codereturn = SendCode(RunnableCode)\n",
    "        if 'error' in codereturn.lower():\n",
    "            self_correct_str = \"Self-correction but still got an error.\\n\\n\"\n",
    "        else:\n",
    "            self_correct_str = \"Self-corrected.\\n\\n\"\n",
    "        \n",
    "        codereturn += self_correct_str\n",
    "        print(self_correct_str)\n",
    "\n",
    "    return codereturn\n",
    "    \n",
    "\n",
    "# Evaluate dataset\n",
    "def EvalDataset():\n",
    "    # Declare the use of the global variable to store user question\n",
    "    global user_question_global\n",
    "\n",
    "    # Define task range\n",
    "    task_infos = range(1, 106)\n",
    "\n",
    "    # Read JSON file\n",
    "    with open(\"./docs/WMX3API_MCEval_Evaluation_Dataset.json\", \"r\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    # Initialize statistics dictionary\n",
    "    statistics = {\n",
    "        1: {'correct': 0, 'syntax_error': 0, 'api_error': 0, 'self_corrected_error': 0, 'self_corrected_correct': 0, 'total_errors': 0, 'total': 0},\n",
    "        2: {'correct': 0, 'syntax_error': 0, 'api_error': 0, 'self_corrected_error': 0, 'self_corrected_correct': 0, 'total_errors': 0, 'total': 0},\n",
    "        3: {'correct': 0, 'syntax_error': 0, 'api_error': 0, 'self_corrected_error': 0, 'self_corrected_correct': 0, 'total_errors': 0, 'total': 0}\n",
    "    }\n",
    "\n",
    "    total_correct = 0\n",
    "    total_syntax_error = 0\n",
    "    total_api_error = 0\n",
    "    total_self_corrected_error = 0\n",
    "    total_self_corrected_correct = 0\n",
    "    total_errors = 0\n",
    "\n",
    "    # Initialize error log list\n",
    "    error_log = []\n",
    "    self_corrected_log = []\n",
    "\n",
    "    start_time = datetime.now()  # Record start time\n",
    "\n",
    "    # Iterate through task range\n",
    "    for task_info in tqdm(task_infos, desc=\"Processing tasks\"):\n",
    "        # Get task information\n",
    "        task_entry = next(item for item in dataset if item[\"TaskId\"] == task_info)\n",
    "        user_question = task_entry[\"Instruction\"]\n",
    "        difficulty = task_entry[\"Difficulty\"]\n",
    "        task_id = task_entry[\"TaskId\"]\n",
    "\n",
    "        print(f\"Task ID: {task_id} ðŸ”½\")\n",
    "        folder_path = f'/Users/yin/Documents/GitHub/MCCodeLog/{llm_name}'\n",
    "        \n",
    "        user_question_global = user_question\n",
    "        # Call coder_router function\n",
    "        NoCoder, coder_router_result = coder_router(user_question)\n",
    "\n",
    "        # Route the result based on NoCoder value\n",
    "        if NoCoder == 0:  # Coding task\n",
    "            tasks = task_decomposer_llm(user_question)\n",
    "            # Initialize a code string from LLM\n",
    "            code_from_llm_str = ''\n",
    "            for i in range(len(tasks)):\n",
    "                coder_return = coder_retrieval(coder_router_result)  # Code context\n",
    "                # Call CoderLLM function\n",
    "                code_from_llm = CoderLLM(user_question, coder_return, task_id)\n",
    "                code_from_llm_str += f'\\n#---------task{i}:---------\\n' + tasks[i] + f'\\n#---------code{i}:---------\\n' + code_from_llm \n",
    "\n",
    "        # Single task\n",
    "        if len(tasks) == 1:\n",
    "            # Run code\n",
    "            CoderResult = RunCode(code_from_llm, task_info)\n",
    "        else:  # Multi tasks\n",
    "            code_from_composer_llm = tasks_composer_llm(user_question, code_from_llm_str)\n",
    "            CoderResult = RunCode(code_from_composer_llm, task_info)\n",
    "\n",
    "        # Init Correctness, if equals 1, then plot.\n",
    "        Correctness = 0\n",
    "        statistics[difficulty]['total'] += 1\n",
    "        # Check for \"Self-correct\" in the result\n",
    "        if 'self-correct' in CoderResult.lower():\n",
    "            self_corrected_log.append({'TaskId': task_info, 'Result': CoderResult})\n",
    "            if 'self-correction but still got an error' in CoderResult.lower():\n",
    "                error_info = {\n",
    "                    'TaskId': task_info,\n",
    "                    'Error': CoderResult\n",
    "                }\n",
    "                error_log.append(error_info)\n",
    "                statistics[difficulty]['total_errors'] += 1\n",
    "                statistics[difficulty]['self_corrected_error'] += 1\n",
    "                total_self_corrected_error += 1\n",
    "                total_errors += 1\n",
    "\n",
    "                if 'syntaxerror' in CoderResult.lower():\n",
    "                    statistics[difficulty]['syntax_error'] += 1\n",
    "                    total_syntax_error += 1\n",
    "                else:\n",
    "                    statistics[difficulty]['api_error'] += 1\n",
    "                    total_api_error += 1\n",
    "            elif 'self-corrected' in CoderResult.lower():\n",
    "                statistics[difficulty]['correct'] += 1\n",
    "                statistics[difficulty]['self_corrected_correct'] += 1\n",
    "                total_self_corrected_correct += 1\n",
    "                total_correct += 1\n",
    "                Correctness = 1\n",
    "        else:\n",
    "            statistics[difficulty]['correct'] += 1\n",
    "            total_correct += 1\n",
    "            Correctness = 1\n",
    "\n",
    "        if Correctness == 1:\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "            # Plot with the log file\n",
    "            log_file_path = os.path.join(folder_path, f\"{task_info}_{llm_name}_log.txt\")\n",
    "            plot_log(log_file_path)\n",
    "            print('# -------------------------------------------------------------------------\\n')\n",
    "\n",
    "    end_time = datetime.now()  # Record end time\n",
    "    total_runtime = end_time - start_time  # Calculate total runtime\n",
    "    total_runtime_str = f\"{total_runtime.seconds // 60}m{total_runtime.seconds % 60}s\"  # Format runtime\n",
    "\n",
    "    # Define the folder path and the file name\n",
    "    file_name = f'{llm_name}_Runlog_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}_{total_runtime_str}.txt'\n",
    "    file_path = f'{folder_path}/{file_name}'\n",
    "\n",
    "    # Open the file in write mode\n",
    "    with open(file_path, 'w') as file:\n",
    "        # Print and write overall statistics\n",
    "        total_tasks = sum([statistics[d]['total'] for d in statistics])\n",
    "        overall_results = (\n",
    "            f\"Overall Results:\\n\"\n",
    "            f\"  Total Correct: {total_correct} ({total_correct / total_tasks:.2%})\\n\"\n",
    "            f\"      Total Self-corrected Correct: {total_self_corrected_correct} ({total_self_corrected_correct / total_tasks:.2%})\\n\"\n",
    "            f\"  Total Errors: {total_errors} ({total_errors / total_tasks:.2%})\\n\"\n",
    "            f\"      Total Syntax Error: {total_syntax_error} ({total_syntax_error / total_tasks:.2%})\\n\"\n",
    "            f\"      Total API Error: {total_api_error} ({total_api_error / total_tasks:.2%})\\n\"\n",
    "            f\"  Total Self-corrected Errors: {total_self_corrected_error} ({total_self_corrected_error / total_tasks:.2%})\\n\\n\"\n",
    "            \n",
    "        )\n",
    "        print(overall_results)\n",
    "        file.write(overall_results)\n",
    "\n",
    "        # Print and write statistics by difficulty\n",
    "        for difficulty, counts in statistics.items():\n",
    "            total_difficulty = counts['total']\n",
    "            if total_difficulty != 0:\n",
    "                difficulty_results = (\n",
    "                    f\"Difficulty: {difficulty}\\n\"\n",
    "                    f\"  Correct: {counts['correct']} ({counts['correct'] / total_difficulty:.2%})\\n\"\n",
    "                    f\"      Self-corrected Correct: {counts['self_corrected_correct']} ({counts['self_corrected_correct'] / total_difficulty:.2%})\\n\"\n",
    "                    f\"  Errors: {counts['total_errors']} ({counts['total_errors'] / total_difficulty:.2%})\\n\"\n",
    "                    f\"      Syntax Error: {counts['syntax_error']} ({counts['syntax_error'] / total_difficulty:.2%})\\n\"\n",
    "                    f\"      API Error: {counts['api_error']} ({counts['api_error'] / total_difficulty:.2%})\\n\"\n",
    "                    f\"  Self-corrected Errors: {counts['self_corrected_error']} ({counts['self_corrected_error'] / total_difficulty:.2%})\\n\\n\"\n",
    "                    \n",
    "                )\n",
    "                print(difficulty_results)\n",
    "                file.write(difficulty_results)\n",
    "\n",
    "        # Print and write error log\n",
    "        if error_log:\n",
    "            error_log_results = \"Error Log:\\n\"\n",
    "            for error in error_log:\n",
    "                error_log_results += f\"  TaskId: {error['TaskId']}, Error: {error['Error']}\\n\"\n",
    "            error_log_results += \"\\n\"\n",
    "            print(error_log_results)\n",
    "            file.write(error_log_results)\n",
    "\n",
    "        # Print and write self-corrected log\n",
    "        if self_corrected_log:\n",
    "            self_corrected_log_results = \"Self-corrected Log:\\n\"\n",
    "            for log in self_corrected_log:\n",
    "                self_corrected_log_results += f\"  TaskId: {log['TaskId']}, Result: {log['Result']}\\n\"\n",
    "            self_corrected_log_results += \"\\n\"\n",
    "            print(self_corrected_log_results)\n",
    "            file.write(self_corrected_log_results)\n",
    "\n",
    "\n",
    "\n",
    "EvalDataset()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-demo-IMu3vKF7-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
